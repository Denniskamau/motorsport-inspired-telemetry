I want to create a project that will help me stand out when I apply to this job:......

AWS Cloud Engineer for Racing Applications (m/w/d)
Don‚Äôt just go to work; become part of a unique company within a fast-moving industry. Toyota Gazoo Racing Europe ‚Äì creating excitement through team spirit and advanced technology!

We are looking for a Cloud Engineer to join our application development and works motorsport team in Cologne, Germany. In this role within the IT department, you will be responsible for developing and maintaining our AWS cloud infrastructure, supporting race team operations and data analytics. Partial remote work is possible.

Are you a team player who thrives on responsibility, with strong analytical and logical skills? Then you are the right person to take on this challenge. Be inspired by the passion and teamwork of our 500 strong workforce for motorsport and automotive projects.

 

What we offer:

exciting projects and a place for technical freedom and innovation to get things moving
attractive benefits packages e.g. competitive remuneration, social benefits, 30 days annual holiday, car leasing, free on-site gym
a challenging, fulfilling workplace in a multi-national company within a familiar atmosphere - driven by fascination and passion
 

Your tasks:

Design, implement, and manage Hybrid AWS infrastructure using Terraform across multiple accounts and regions.
Take part in crucial software architecture decisions.
Maintain secure and scalable hybrid setups between Edge, Cloud and On-Prem.
Maintain critical infrastructure in the AWS Cloud.
Support CI/CD pipelines and Infrastructure automation for internal software applications.
Collaborate with software developers and engineers to enable secure cloud-native solutions for motor-sport analytics, APIs, and AI workloads.
Possibilities to participate in the full software development life cycle.
 

The successful candidate will:

Have a Bachelor/master‚Äôs degree or equivalent in computer science, cloud engineering, or a related field.
Bring 3+ years of hands-on experience with AWS and Infrastructure as Code (IAC) in a production environment.
Be familiar with AWS services such as EKS, ECS, VPC, EC2, IAM, CloudTrail, Athena, RDS, ECR, and S3.
Understand CI/CD workflows and DevOps practices in cloud-native environments.
Have experience with containerized workloads (K8s and Docker).
Be fluent in English; German language skills are a plus.
Have experience with one or more of the following would be an advantage: Terraform, Azure DevOps Pipelines, Python, AI/ML workloads, Data Engineering, Networking
Be available to work on-site in Cologne, Germany at least 3 days per week. 



Project idea... 

üß© Architecture (high level)

F1 Data Source (API / CSV)
        ‚Üì
Edge Simulator (container)
        ‚Üì
AWS Ingestion (API Gateway / ALB)
        ‚Üì
EKS (processing & APIs)
        ‚Üì
S3 (raw + processed data)
        ‚Üì
Athena / Glue
        ‚Üì
Dashboards (Grafana)

Why this project works for motorsport roles

This directly demonstrates:
	‚Ä¢	Hybrid thinking (edge ‚Üí cloud)
	‚Ä¢	Race-weekend reliability
	‚Ä¢	Data latency awareness
	‚Ä¢	Cloud + Kubernetes mastery
	‚Ä¢	Infrastructure as Code
	‚Ä¢	Observability

It screams:

‚ÄúI understand race operations, not just Kubernetes.‚Äù

‚∏ª

Data source (easy + real)

Use Ergast F1 API (free, no auth):
	‚Ä¢	Laps
	‚Ä¢	Drivers
	‚Ä¢	Constructors
	‚Ä¢	Race results
	‚Ä¢	Pit stops
	‚Ä¢	Qualifying

Example:
http://ergast.com/api/f1/current/last/laps.json


Core components (what you actually implement)

1Ô∏è‚É£ Edge simulator (Docker)

Simulate trackside systems.
	‚Ä¢	Python container
	‚Ä¢	Periodically pulls F1 data
	‚Ä¢	Pushes to cloud endpoint
	‚Ä¢	Adds artificial latency + packet loss (optional)

Tech:
	‚Ä¢	Python
	‚Ä¢	Docker
	‚Ä¢	Deployed as:
	‚Ä¢	Local container OR
	‚Ä¢	EKS DaemonSet (nice touch)

‚∏ª

2Ô∏è‚É£ AWS Infrastructure (Terraform)

This is where you shine.

Provision:
	‚Ä¢	VPC (private subnets)
	‚Ä¢	EKS cluster
	‚Ä¢	ALB Ingress Controller
	‚Ä¢	IAM Roles for Service Accounts (IRSA)
	‚Ä¢	S3 buckets:
	‚Ä¢	raw-telemetry
	‚Ä¢	processed-telemetry
	‚Ä¢	CloudWatch + Prometheus

This alone maps 1:1 to their JD.

‚∏ª

3Ô∏è‚É£ Data ingestion service (Kubernetes)
	‚Ä¢	Small FastAPI service
	‚Ä¢	Receives telemetry
	‚Ä¢	Writes raw data to S3
	‚Ä¢	Pushes metrics

Run as:
	‚Ä¢	Deployment + HPA
	‚Ä¢	Proper resource limits
	‚Ä¢	PodDisruptionBudgets

‚∏ª

4Ô∏è‚É£ Analytics pipeline

Keep it simple but realistic:
	‚Ä¢	Glue crawler for schema
	‚Ä¢	Athena queries for:
	‚Ä¢	Lap times
	‚Ä¢	Sector trends
	‚Ä¢	Pit stop deltas
	‚Ä¢	Optional:
	‚Ä¢	Spark job
	‚Ä¢	Python batch job in EKS

‚∏ª

5Ô∏è‚É£ Dashboards (this is what recruiters LOVE)

Grafana dashboards showing:
	‚Ä¢	Lap time deltas
	‚Ä¢	Driver consistency
	‚Ä¢	Ingestion latency
	‚Ä¢	Error rates
	‚Ä¢	Throughput per minute

Bonus:
	‚Ä¢	‚ÄúRace weekend mode‚Äù dashboard

‚∏ª

What you explicitly document (very important)

Create a README.md that explains:

üö¶ ‚ÄúRace weekend workflow‚Äù
	‚Ä¢	When infra is frozen
	‚Ä¢	How scaling is handled
	‚Ä¢	How failures are mitigated

üîí Security
	‚Ä¢	IAM boundaries
	‚Ä¢	IRSA
	‚Ä¢	No static credentials

üîÅ CI/CD
	‚Ä¢	GitHub Actions / Azure DevOps
	‚Ä¢	Terraform plan/apply
	‚Ä¢	Helm deploy
