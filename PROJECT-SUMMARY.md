# üéâ Project Complete - F1 Telemetry Platform

## ‚úÖ What We Built

A **production-ready, cloud-native telemetry platform** specifically designed to showcase your skills for the **Toyota Gazoo Racing Europe - AWS Cloud Engineer** position.

### üèóÔ∏è Complete Implementation

#### 1. **Edge Simulator** ‚úÖ
- Python application simulating trackside telemetry collection
- Collects real F1 data from Ergast API
- Network condition simulation (latency, packet loss)
- Containerized with Docker
- **Location**: `edge-simulator/`

#### 2. **Ingestion Service** ‚úÖ
- FastAPI REST API for telemetry ingestion
- S3/MinIO compatible storage
- Prometheus metrics export
- Health checks and proper error handling
- **Location**: `ingestion-service/`

#### 3. **AWS Infrastructure (Terraform)** ‚úÖ
- **VPC Module**: Multi-AZ networking with NAT Gateways
- **EKS Module**: Managed Kubernetes with OIDC/IRSA
- **S3 Module**: Three buckets with lifecycle policies
- **IAM Module**: Least-privilege roles with IRSA
- **Location**: `terraform/`

#### 4. **Kubernetes Manifests** ‚úÖ
- Production manifests for AWS EKS
- Local manifests for kind
- HPA (Horizontal Pod Autoscaler)
- PDB (Pod Disruption Budget)
- Proper resource limits
- **Locations**: `k8s/` (AWS), `local-dev/k8s/` (local)

#### 5. **Analytics Pipeline** ‚úÖ
- AWS Glue crawler configuration
- Athena SQL queries for analytics
- Sample queries for lap times, pit stops, data quality
- **Location**: `analytics/`

#### 6. **Monitoring & Observability** ‚úÖ
- Prometheus configuration
- Grafana dashboards (2 dashboards)
  - Telemetry Overview
  - Race Weekend Mode
- Custom application metrics
- **Location**: `k8s/grafana/`, `local-dev/`

#### 7. **CI/CD Pipelines** ‚úÖ
- Terraform deployment automation
- Docker build and security scanning
- Kubernetes deployment automation
- **Location**: `.github/workflows/`

#### 8. **Local Development Environment** ‚úÖ
- Docker Compose for MinIO, Prometheus, Grafana
- kind cluster configuration (3 nodes)
- One-command setup script
- Demo and testing scripts
- **Location**: `local-dev/`

## üöÄ How to Use This Project

### For Your Demo (Local - No AWS Cost!)

```bash
# 1. Clone and setup
cd motorsport-inspired-telemetry/local-dev/scripts
./setup.sh

# 2. Access services (wait 3-5 min for setup)
# - Ingestion API: http://localhost:30080
# - Prometheus: http://localhost:30090
# - Grafana: http://localhost:30030 (admin/admin)
# - MinIO: http://localhost:9001 (minioadmin/minioadmin)

# 3. Run demo
./demo.sh

# 4. Test manually
./test-ingestion.sh

# 5. When done
./teardown.sh
```

### For AWS Deployment (When Ready)

```bash
# 1. Deploy infrastructure
cd terraform
terraform init
terraform apply -var-file=environments/dev/terraform.tfvars

# 2. Configure kubectl
aws eks update-kubeconfig --region us-east-1 --name f1-telemetry-dev

# 3. Deploy applications
kubectl apply -f k8s/ingestion-service/
kubectl apply -f k8s/edge-simulator/
```

## üìñ Documentation Created

| Document | Purpose | Location |
|----------|---------|----------|
| **README.md** | Main project overview | Root |
| **DEMO-GUIDE.md** | Step-by-step demo script | Root |
| **OBJECTIVE.MD** | Original requirements | Root |
| **local-dev/README.md** | Local setup guide | local-dev/ |
| **terraform/README.md** | Infrastructure guide | terraform/ |
| **k8s/README.md** | Kubernetes guide | k8s/ |
| **analytics/README.md** | Analytics guide | analytics/ |
| Service READMEs | Individual components | edge-simulator/, ingestion-service/, etc. |

## üé¨ Recording Your Demo

### Quick Setup (15 minutes before recording)

```bash
# Start everything
cd local-dev/scripts
./setup.sh

# Prepare browser tabs
open http://localhost:30030  # Grafana
open http://localhost:30090  # Prometheus
open http://localhost:9001   # MinIO

# Prepare terminals (3 windows)
# Terminal 1: kubectl logs -f deployment/ingestion-service
# Terminal 2: kubectl logs -f deployment/edge-simulator
# Terminal 3: watch kubectl get pods
```

### Demo Script (Follow DEMO-GUIDE.md)

1. **Introduction** (2 min) - Explain project and motorsport context
2. **Live Demo** (5 min) - Show running system, logs, metrics
3. **Code Review** (2 min) - Terraform, K8s manifests, Python code
4. **Production Readiness** (1 min) - CI/CD, monitoring, analytics

### Key Points to Emphasize

‚úÖ **Technical**: IRSA, HPA, PDB, Terraform modules, Prometheus metrics
‚úÖ **Motorsport**: Race weekend operations, edge simulation, reliability
‚úÖ **Job Fit**: Matches every requirement from the job description

## üéØ Skills Demonstrated

### From Job Description

| Requirement | Demonstrated |
|-------------|--------------|
| AWS (EKS, VPC, S3, IAM, etc.) | ‚úÖ Complete Terraform infrastructure |
| Infrastructure as Code | ‚úÖ Terraform modules for all components |
| Kubernetes | ‚úÖ EKS cluster, HPA, PDB, IRSA |
| Docker | ‚úÖ Multi-stage builds, optimization |
| CI/CD | ‚úÖ GitHub Actions workflows |
| Python | ‚úÖ FastAPI service, edge simulator |
| Data Engineering | ‚úÖ S3 partitioning, Glue, Athena |
| Networking | ‚úÖ VPC design, security groups |

## üìä Project Statistics

- **Total Files Created**: 50+
- **Lines of Code**: ~5,000
- **Terraform Modules**: 4 (VPC, EKS, S3, IAM)
- **Docker Images**: 2 (edge-simulator, ingestion-service)
- **Kubernetes Manifests**: 15+
- **Documentation**: 10+ README files
- **Scripts**: 5 automation scripts
- **CI/CD Pipelines**: 3 GitHub Actions workflows

## üîç What Makes This Project Stand Out

1. **Motorsport Context** - Not just generic cloud infrastructure, but specifically designed for race operations
2. **Production Ready** - HPA, PDB, IRSA, proper observability - not a toy project
3. **Complete Stack** - Edge to cloud, infrastructure to analytics, CI/CD to monitoring
4. **Local Demo** - Can be demonstrated without AWS costs
5. **Documentation** - Comprehensive guides for every component
6. **Job-Specific** - Directly addresses every requirement in the Toyota Gazoo Racing job description

## üö¶ Next Steps

### Before Your Demo

- [ ] Test the setup script end-to-end
- [ ] Practice your demo (aim for 8-12 minutes)
- [ ] Prepare answers for common questions
- [ ] Test recording software (OBS Studio recommended)
- [ ] Ensure good audio/video quality

### For GitHub

```bash
# Initialize git repo
git init
git add .
git commit -m "Initial commit: F1 Telemetry Platform

Complete cloud-native telemetry platform demonstrating:
- Hybrid edge-to-cloud architecture
- AWS infrastructure with Terraform
- Kubernetes with auto-scaling and high availability
- Data analytics pipeline
- Production-ready observability
- CI/CD automation

Built for Toyota Gazoo Racing Europe - AWS Cloud Engineer application"

# Push to GitHub
git remote add origin <your-repo-url>
git push -u origin main
```

### For Your Application

1. **GitHub README**: The main README.md is already optimized for GitHub
2. **Portfolio Website**: Link to GitHub repo with screenshots
3. **LinkedIn**: Post about the project with key highlights
4. **Cover Letter**: Reference specific components that match job requirements
5. **Demo Video**: Upload to YouTube/Loom and share link

## üí° Demo Tips

### What to Say

**Opening:**
> "I built this F1 telemetry platform to demonstrate my understanding of cloud engineering for motorsport applications. It simulates edge devices at the track sending data to cloud infrastructure - similar to what Toyota Gazoo Racing uses during race weekends."

**During Demo:**
- "This HPA ensures we can scale from 2 to 5 replicas based on load - crucial for race weekends"
- "Using IRSA means no static credentials anywhere - following AWS security best practices"
- "The PDB ensures minimum availability even during node maintenance"
- "Data is partitioned in S3 for cost-effective Athena queries"

**Closing:**
> "This project demonstrates all the key skills from the job description: AWS services, Terraform, Kubernetes, Docker, CI/CD, Python, and data engineering. But more importantly, it shows I understand race operations and reliability requirements, not just cloud technology."

## üÜò Troubleshooting

If something goes wrong during setup:

```bash
# Check Docker is running
docker ps

# Check kind cluster
kind get clusters

# Check pods
kubectl get pods --all-namespaces

# View logs
kubectl logs -n default <pod-name>

# Nuclear option - restart everything
./teardown.sh
./setup.sh
```

## üìß Questions?

If you run into issues:
1. Check the relevant README.md file
2. Look at the DEMO-GUIDE.md for quick fixes
3. Review logs: `kubectl logs <pod-name>`

## üéä Congratulations!

You now have a **complete, production-ready portfolio project** that:
- Demonstrates advanced cloud engineering skills
- Shows understanding of motorsport operations
- Can be run locally without AWS costs
- Is ready for demonstrations and interviews
- Matches the Toyota Gazoo Racing job description perfectly

**Good luck with your application! üèéÔ∏èüí®**

---

*"I understand race operations, not just Kubernetes."*
